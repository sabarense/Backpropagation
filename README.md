# ğŸ§  Backpropagation para FunÃ§Ãµes LÃ³gicas

ImplementaÃ§Ã£o do algoritmo **Backpropagation** para resolver funÃ§Ãµes lÃ³gicas **AND**, **OR** e **XOR**, com suporte para mÃºltiplas entradas booleanas. Este projeto tem carÃ¡ter educacional e visa ilustrar os fundamentos do aprendizado supervisionado com redes neurais multicamadas.

---

## ğŸ“‹ Ãndice

- [ğŸ“– DescriÃ§Ã£o](#-descriÃ§Ã£o)
- [âœ¨ CaracterÃ­sticas](#-caracterÃ­sticas)
- [ğŸš€ InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ğŸ’» Como Usar](#-como-usar)
- [ğŸ“Š Resultados](#-resultados)
- [ğŸ§  AnÃ¡lise TeÃ³rica](#-anÃ¡lise-teÃ³rica)
- [ğŸ“ LicenÃ§a](#-licenÃ§a)

---

## ğŸ“– DescriÃ§Ã£o

O **Backpropagation** Ã© o algoritmo padrÃ£o para treinar redes neurais multicamadas ajustando pesos por meio do cÃ¡lculo do gradiente do erro.

Este projeto demonstra:

- Como uma rede neural com camada oculta resolve funÃ§Ãµes lÃ³gicas **lineares** e **nÃ£o lineares** (AND, OR e XOR)
- A importÃ¢ncia da **taxa de aprendizado**, **bias** e **funÃ§Ã£o de ativaÃ§Ã£o** no aprendizado
- A evoluÃ§Ã£o dos **pesos**, **bias** e acurÃ¡cia durante o treinamento
- VisualizaÃ§Ã£o da fronteira de decisÃ£o para problemas com 2 entradas

---

## âœ¨ CaracterÃ­sticas

- âœ… Suporte para **n entradas booleanas** (2 a 10)
- âœ… ImplementaÃ§Ã£o da rede neural com uma camada oculta
- âœ… Escolha da funÃ§Ã£o lÃ³gica (**AND**, **OR**, **XOR**)
- âœ… InvestigaÃ§Ã£o da influÃªncia de parÃ¢metros:
  - Taxa de aprendizado
  - Uso do bias
  - FunÃ§Ãµes de ativaÃ§Ã£o (Sigmoide, Tangente HiperbÃ³lica)
- âœ… VisualizaÃ§Ã£o grÃ¡fica da fronteira de decisÃ£o para 2 entradas
- âœ… EvoluÃ§Ã£o dos pesos, bias e acurÃ¡cia durante o treinamento

---

## ğŸš€ InstalaÃ§Ã£o

### âœ… PrÃ©-requisitos

- Python 3.7 ou superior

### ğŸ“¦ Instale as dependÃªncias:

```bash
pip install numpy matplotlib
```

###ğŸ”„ Clonando o repositÃ³rio:

git clone https://github.com/sabarense/Backpropagation.git

cd Backpropagation

pip install -r requirements.txt

###ğŸ’» Como Usar

```bash
python backpropagation.py
```

No terminal, vocÃª serÃ¡ solicitado a informar:

Qual funÃ§Ã£o lÃ³gica deseja treinar (AND, OR, XOR)

Quantidade de entradas (mÃ­nimo 2, mÃ¡ximo 10)

Taxa de aprendizado

Uso ou nÃ£o do bias

FunÃ§Ã£o de ativaÃ§Ã£o (sigmoid ou tanh)

Se forem escolhidas 2 entradas, serÃ¡ exibida uma visualizaÃ§Ã£o animada da evoluÃ§Ã£o da fronteira de decisÃ£o.

### ğŸ“Š Resultados

ğŸ”¹ Taxa de aprendizado

        - Taxas muito baixas tornam o treinamento lento e podem nÃ£o convergir em poucas Ã©pocas.
        
        - Taxas muito altas causam instabilidade.
        
        - Taxa ideal encontrada: 0.1

ğŸ”¹ Uso do bias

        - Essencial para o correto aprendizado de funÃ§Ãµes nÃ£o linearmente separÃ¡veis, como o XOR.

ğŸ”¹ FunÃ§Ãµes de ativaÃ§Ã£o

        - Sigmoide: Tradicional, mas com saturaÃ§Ã£o em valores extremos.
        - Tangente HiperbÃ³lica (tanh): Melhor desempenho devido Ã  saÃ­da centrada em zero.

## ğŸ§  AnÃ¡lise TeÃ³rica

O algoritmo Backpropagation ajusta os pesos da rede minimizando o erro da saÃ­da em relaÃ§Ã£o Ã  saÃ­da esperada, utilizando o gradiente descendente.

A atualizaÃ§Ã£o dos pesos segue a regra geral:

````
w â† w + Î· * Î´ * entrada

Onde:

Î· Ã© a taxa de aprendizado

Î´ Ã© o erro local calculado a partir da funÃ§Ã£o de ativaÃ§Ã£o e do erro da camada seguinte

````

### ğŸ“ LicenÃ§a
Este projeto Ã© de uso educacional e estÃ¡ sob a licenÃ§a MIT.
Sinta-se Ã  vontade para estudar, modificar e reutilizar o cÃ³digo.